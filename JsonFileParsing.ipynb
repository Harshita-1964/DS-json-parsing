{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f809b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Type: Regression\n",
      "Target Variable: petal_width\n",
      "Regression Type: regression\n",
      "Partitioning: True\n"
     ]
    }
   ],
   "source": [
    "#Read the target and type of regression to be run\n",
    "import json\n",
    "\n",
    "with open('algoparams_from_ui.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "target = data['design_state_data']['target']\n",
    "prediction_type = target['prediction_type']\n",
    "target_variable = target['target']\n",
    "regression_type = target['type']\n",
    "partitioning = target['partitioning']\n",
    "\n",
    "# Print the extracted information\n",
    "print(f\"Prediction Type: {prediction_type}\")\n",
    "print(f\"Target Variable: {target_variable}\")\n",
    "print(f\"Regression Type: {regression_type}\")\n",
    "print(f\"Partitioning: {partitioning}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d1ed219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width      species\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "#Reading the features\n",
    "import pandas as pd\n",
    "feature_handling = data['design_state_data']['feature_handling'] #Extract feature handling information\n",
    "\n",
    "df = pd.read_csv('iris_modified.csv')\n",
    "\n",
    "for feature_name, feature_info in feature_handling.items():\n",
    "    if feature_info['is_selected']:\n",
    "        feature_variable_type = feature_info['feature_variable_type']\n",
    "        feature_details = feature_info.get('feature_details', {})\n",
    "        missing_values = feature_details.get('missing_values', 'N/A')\n",
    "        impute_with = feature_details.get('impute_with', 'N/A')\n",
    "        impute_value = feature_details.get('impute_value', 'N/A')\n",
    "\n",
    "        if missing_values == 'Impute':\n",
    "            if impute_with == 'Average of values':\n",
    "                if feature_variable_type == 'numerical':\n",
    "                    df[feature_name].fillna(df[feature_name].mean(), inplace=True)\n",
    "                else:\n",
    "                    pass\n",
    "            elif impute_with == 'Custom value':\n",
    "                df[feature_name].fillna(impute_value, inplace=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612e5de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width      species\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "#feature reduction based on input\n",
    "feature_reduction = data['design_state_data']['feature_reduction']\n",
    "\n",
    "if isinstance(feature_reduction, dict):\n",
    "    for method, method_info in feature_reduction.items():\n",
    "        if isinstance(method_info, dict) and method_info.get('is_selected', False):\n",
    "            num_of_features_to_keep = method_info['num_of_features_to_keep']\n",
    "\n",
    "            if method == 'No Reduction':\n",
    "                pass\n",
    "            elif method == 'Correlation with target':\n",
    "                pass\n",
    "            elif method == 'Tree-based':\n",
    "                pass\n",
    "            elif method == 'Principal Component Analysis': # Implementing PCA-based feature reduction\n",
    "                pass\n",
    "\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Invalid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ff59ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=[0.5, 0.6, 0.7, 0.8], l1_ratio=[0.5, 0.6, 0.7, 0.8],\n",
      "                   max_iter=50, n_jobs=2, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Parsing JSON to extract the model parameters given in the file\n",
    "logistic_regression_params = {\n",
    "    \"model_name\": \"Logistic Regression\",\n",
    "    \"is_selected\": True, \n",
    "    \"parallelism\": 2,\n",
    "    \"min_iter\": 30,\n",
    "    \"max_iter\": 50,\n",
    "    \"min_regparam\": 0.5,\n",
    "    \"max_regparam\": 0.8,\n",
    "    \"min_elasticnet\": 0.5,\n",
    "    \"max_elasticnet\": 0.8\n",
    "}\n",
    "\n",
    "if logistic_regression_params[\"is_selected\"]:\n",
    "    # Creating Logistic Regression as model object\n",
    "    logistic_regression_model = LogisticRegression(\n",
    "        random_state=0,\n",
    "        max_iter=logistic_regression_params[\"max_iter\"],\n",
    "        solver=\"lbfgs\",\n",
    "        multi_class=\"auto\",\n",
    "        n_jobs=logistic_regression_params[\"parallelism\"],\n",
    "        C=[i/10 for i in range(int(logistic_regression_params[\"min_regparam\"]*10),\n",
    "                                int((logistic_regression_params[\"max_regparam\"] + 0.1)*10))],\n",
    "        l1_ratio=[i/10 for i in range(int(logistic_regression_params[\"min_elasticnet\"]*10),\n",
    "                                      int((logistic_regression_params[\"max_elasticnet\"] + 0.1)*10))]\n",
    "    )\n",
    "\n",
    "    print(logistic_regression_model)\n",
    "else:\n",
    "    print(\"Logistic Regression is not selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7a1a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(n_jobs=2)\n"
     ]
    }
   ],
   "source": [
    "#sample for logistic regression as well\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_regression_params = {\n",
    "    \"model_name\": \"Linear Regression\",\n",
    "    \"is_selected\": True,  # Change this to True to select the model\n",
    "    \"parallelism\": 2,\n",
    "    \"min_iter\": 30,\n",
    "    \"max_iter\": 50,\n",
    "}\n",
    "\n",
    "if linear_regression_params[\"is_selected\"]:\n",
    "    linear_regression_model = LinearRegression(\n",
    "        n_jobs=linear_regression_params[\"parallelism\"]\n",
    "    )\n",
    "    print(linear_regression_model)\n",
    "else:\n",
    "    print(\"Linear Regression is not selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a1f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Displaying the selected features\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "selected_features = X.columns[selected_feature_indices]\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebac1e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ddcb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Feature Handling Parameters: {'sepal_length': {'feature_name': 'sepal_length', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'StandardScaler', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'Average of values', 'impute_value': 0}}, 'sepal_width': {'feature_name': 'sepal_width', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'No rescaling', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'custom', 'impute_value': -1}}, 'petal_length': {'feature_name': 'petal_length', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'No rescaling', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'Average of values', 'impute_value': 0}}, 'petal_width': {'feature_name': 'petal_width', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'No rescaling', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'custom', 'impute_value': -2}}, 'species': {'feature_name': 'species', 'is_selected': True, 'feature_variable_type': 'text', 'feature_details': {'text_handling': 'Tokenize and hash', 'hash_columns': 0}}}\n",
      "Processing feature: sepal_length\n",
      "Processing feature: sepal_width\n",
      "Processing feature: petal_length\n",
      "Processing feature: petal_width\n",
      "Processing feature: species\n",
      "Shape of the processed data: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_feature_handling_pipeline(features, json_data):\n",
    "    # Initializing an empty list to store the processing steps\n",
    "    steps = []\n",
    "\n",
    "    if \"feature_handling\" in json_data:\n",
    "        feature_handling_params = json_data[\"feature_handling\"]\n",
    "        print(\"Selected Features:\", features)\n",
    "        print(\"Feature Handling Parameters:\", feature_handling_params)\n",
    "\n",
    "        for feature, details in feature_handling_params.items():\n",
    "            if details.get(\"is_selected\", False):\n",
    "                print(f\"Processing feature: {feature}\")\n",
    "\n",
    "                # Handling missing values\n",
    "                feature_details = details.get(\"feature_details\", {})\n",
    "                if feature_details.get(\"missing_values\") == \"Impute\":\n",
    "                    impute_strategy = feature_details.get(\"impute_with\", \"Average of values\")\n",
    "                    if impute_strategy == \"Average of values\":\n",
    "                        imputer = SimpleImputer(strategy=\"mean\")\n",
    "                    elif impute_strategy == \"custom\":\n",
    "                        impute_value = feature_details.get(\"impute_value\", 0)\n",
    "                        imputer = SimpleImputer(strategy=\"constant\", fill_value=impute_value)\n",
    "                    steps.append((f\"{feature}_imputer\", imputer))\n",
    "\n",
    "                # Rescaling\n",
    "                if feature_details.get(\"rescaling\") == \"StandardScaler\":\n",
    "                    scaler = StandardScaler()\n",
    "\n",
    "                    steps.append((f\"{feature}_scaler\", scaler))\n",
    "\n",
    "        # Creating the pipeline\n",
    "        feature_handling_pipeline = Pipeline(steps)\n",
    "\n",
    "        return feature_handling_pipeline\n",
    "    else:\n",
    "        print(\"'feature_handling' key not found in json_data.\")\n",
    "        return None\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "json_data = {\n",
    "    \"feature_handling\": {\n",
    "        \"sepal_length\": {\n",
    "            \"feature_name\": \"sepal_length\",\n",
    "            \"is_selected\": True,\n",
    "            \"feature_variable_type\": \"numerical\",\n",
    "            \"feature_details\": {\n",
    "                \"numerical_handling\": \"Keep as regular numerical feature\",\n",
    "                \"rescaling\": \"StandardScaler\",\n",
    "                \"make_derived_feats\": False,\n",
    "                \"missing_values\": \"Impute\",\n",
    "                \"impute_with\": \"Average of values\",\n",
    "                \"impute_value\": 0\n",
    "            }\n",
    "        },\n",
    "        \"sepal_width\": {\n",
    "            \"feature_name\": \"sepal_width\",\n",
    "            \"is_selected\": True,\n",
    "            \"feature_variable_type\": \"numerical\",\n",
    "            \"feature_details\": {\n",
    "                \"numerical_handling\": \"Keep as regular numerical feature\",\n",
    "                \"rescaling\": \"No rescaling\",\n",
    "                \"make_derived_feats\": False,\n",
    "                \"missing_values\": \"Impute\",\n",
    "                \"impute_with\": \"custom\",\n",
    "                \"impute_value\": -1\n",
    "            }\n",
    "        },\n",
    "        \"petal_length\": {\n",
    "            \"feature_name\": \"petal_length\",\n",
    "            \"is_selected\": True,\n",
    "            \"feature_variable_type\": \"numerical\",\n",
    "            \"feature_details\": {\n",
    "                \"numerical_handling\": \"Keep as regular numerical feature\",\n",
    "                \"rescaling\": \"No rescaling\",\n",
    "                \"make_derived_feats\": False,\n",
    "                \"missing_values\": \"Impute\",\n",
    "                \"impute_with\": \"Average of values\",\n",
    "                \"impute_value\": 0\n",
    "            }\n",
    "        },\n",
    "        \"petal_width\": {\n",
    "            \"feature_name\": \"petal_width\",\n",
    "            \"is_selected\": True,\n",
    "            \"feature_variable_type\": \"numerical\",\n",
    "            \"feature_details\": {\n",
    "                \"numerical_handling\": \"Keep as regular numerical feature\",\n",
    "                \"rescaling\": \"No rescaling\",\n",
    "                \"make_derived_feats\": False,\n",
    "                \"missing_values\": \"Impute\",\n",
    "                \"impute_with\": \"custom\",\n",
    "                \"impute_value\": -2\n",
    "            }\n",
    "        },\n",
    "        \"species\": {\n",
    "            \"feature_name\": \"species\",\n",
    "            \"is_selected\": True,\n",
    "            \"feature_variable_type\": \"text\",\n",
    "            \"feature_details\": {\n",
    "                \"text_handling\": \"Tokenize and hash\",\n",
    "                \"hash_columns\": 0\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Creating the feature handling pipeline\n",
    "feature_pipeline = create_feature_handling_pipeline(selected_features, json_data)\n",
    "\n",
    "if feature_pipeline is not None:\n",
    "    # Fit and transform the data\n",
    "    X_processed = feature_pipeline.fit_transform(X)\n",
    "    print(\"Shape of the processed data:\", X_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f3e66e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_handling': {'sepal_length': {'feature_name': 'sepal_length', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'StandardScaler', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'Average of values', 'impute_value': 0}}, 'sepal_width': {'feature_name': 'sepal_width', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'No rescaling', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'custom', 'impute_value': -1}}, 'petal_length': {'feature_name': 'petal_length', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'No rescaling', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'Average of values', 'impute_value': 0}}, 'petal_width': {'feature_name': 'petal_width', 'is_selected': True, 'feature_variable_type': 'numerical', 'feature_details': {'numerical_handling': 'Keep as regular numerical feature', 'rescaling': 'No rescaling', 'make_derived_feats': False, 'missing_values': 'Impute', 'impute_with': 'custom', 'impute_value': -2}}, 'species': {'feature_name': 'species', 'is_selected': True, 'feature_variable_type': 'text', 'feature_details': {'text_handling': 'Tokenize and hash', 'hash_columns': 0}}}}\n"
     ]
    }
   ],
   "source": [
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a150fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.00019073333333334577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y = df['petal_width']#Target variable\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac7b9ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Squared Error: 6.804952470168233e-32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y = df['petal_width']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression - Mean Squared Error: {mse_linear}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a27523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - Mean Squared Error: 0.0005079011076312915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y = df['petal_width']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "ridge_model = Ridge(alpha=1.0) \n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f\"Ridge Regression - Mean Squared Error: {mse_ridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61e2bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - Mean Squared Error: 0.6421895833333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y = df['petal_width']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "lasso_model = Lasso(alpha=1.0, random_state=42)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"Lasso Regression - Mean Squared Error: {mse_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6fac7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor - Mean Squared Error: 0.00019073333333334577\n",
      "LinearRegression - Mean Squared Error: 6.804952470168233e-32\n",
      "Ridge - Mean Squared Error: 0.0005079011076312915\n",
      "Lasso - Mean Squared Error: 0.6421895833333334\n",
      "ElasticNet - Mean Squared Error: 0.36844374044121003\n",
      "DecisionTreeRegressor - Mean Squared Error: 0.0006666666666666663\n",
      "SVR - Mean Squared Error: 0.009189235166194902\n",
      "KNeighborsRegressor - Mean Squared Error: 0.013586666666666644\n",
      "GradientBoostingRegressor - Mean Squared Error: 1.503453013120862e-05\n",
      "ExtraTreesRegressor - Mean Squared Error: 3.2666666666672484e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor - Mean Squared Error: 0.027049582397114413\n",
      "XGBRegressor - Mean Squared Error: 3.04367516704188e-06\n"
     ]
    }
   ],
   "source": [
    "#Finding mse bulkly \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "y = df['petal_width']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining the models\n",
    "models = {\n",
    "    'RandomForestRegressor': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'SVR': SVR(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'MLPRegressor': MLPRegressor(),\n",
    "    'XGBRegressor': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{model_name} - Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "841854e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 5, 'n_estimators': 30}\n",
      "Mean Squared Error: 0.0010957555710707839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [20, 25],\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Instantiate the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_regressor = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Now you can use the best model for predictions\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f2c3c",
   "metadata": {},
   "source": [
    "Running the fit and prediction of each model, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb3108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "Accuracy: 1.0\n",
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "y_classes = np.digitize(y_train, bins=[-np.inf, 10, 20, np.inf], right=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_classes, test_size=0.2, random_state=42)\n",
    "\n",
    "#parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [20, 30],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Instantiating the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Instantiating the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Printing the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e7844ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 67, 'subsample': 1.0}\n",
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "#performing the same procedure for the remaining algorith9ms\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [67, 89],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_depth': [5, 7],\n",
    "    'subsample': [1.0, 0.9]\n",
    "}\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_regressor = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb70ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'copy_X': True, 'fit_intercept': True, 'n_jobs': 2, 'positive': False}\n",
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'positive': [False],  # 'positive' is the equivalent of 'normalize' in this context\n",
    "    'copy_X': [True, False],\n",
    "    'n_jobs': [2],\n",
    "}\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=linear_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_linear_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = best_linear_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d0fa17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'max_iter': 50, 'solver': 'sag'}\n",
      "Accuracy on the test set: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['species'] = le.fit_transform(df['species'])\n",
    "\n",
    "X = df.drop('species', axis=1)\n",
    "y = df['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [50, 100, 200],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Accuracy on the test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa394cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'max_iter': 30}\n",
      "Mean Squared Error on Test Set: 0.06670937073995876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lasso_model = Lasso()\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Add more values as needed\n",
    "    'max_iter': [30, 50, 100],  # Add more values as needed\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lasso_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_lasso_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_lasso_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "712156cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 30}\n",
      "Mean Squared Error on Test Set: 0.04962916753855276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "elastic_net_model = ElasticNet()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Add more values as needed\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],  # Add more values as needed\n",
    "    'max_iter': [30, 50, 100],  # Add more values as needed\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=elastic_net_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_elastic_net_model = grid_search.best_estimator_\n",
    "y_pred = best_elastic_net_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3c23263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.67, 'gamma': 0.68, 'learning_rate': 0.76, 'max_depth': 56, 'min_child_weight': 0.67, 'n_estimators': 200, 'reg_alpha': 0.77, 'reg_lambda': 0.78, 'subsample': 0.67}\n",
      "Mean Squared Error on Test Set: 0.020483259185628915\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xgboost_model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [56, 89],\n",
    "    'learning_rate': [0.89, 0.76],\n",
    "    'reg_alpha': [0.77],\n",
    "    'reg_lambda': [0.78],\n",
    "    'gamma': [0.68],\n",
    "    'min_child_weight': [0.67],\n",
    "    'subsample': [0.67],\n",
    "    'colsample_bytree': [0.67],\n",
    "    'n_estimators': [100, 200, 300],  # Add more values as needed\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgboost_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_xgboost_model = grid_search.best_estimator_\n",
    "y_pred = best_xgboost_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "883dcd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'absolute_error', 'max_depth': 4, 'min_samples_leaf': 6, 'min_samples_split': 6, 'splitter': 'random'}\n",
      "Mean Squared Error on Test Set: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your data in X_train, X_test, y_train, y_test\n",
    "\n",
    "# Define the Decision Tree Regressor model\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'min_samples_split': [12, 6],\n",
    "    'min_samples_leaf': [12, 6],\n",
    "    'max_depth': [4, 7],\n",
    "    'criterion': ['friedman_mse', 'squared_error', 'absolute_error', 'poisson'],\n",
    "    'splitter': ['best', 'random'],\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_decision_tree_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_decision_tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a77479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Accuracy on Test Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "accuracy = best_estimator.score(X_test, y_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dc10d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 566, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 15, 'tol': 1e-07}\n",
      "Accuracy on Test Set: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "svm_model = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'C': [566, 79],\n",
    "    'gamma': ['scale', 'auto', 'custom'],\n",
    "    'tol': [1e-7, 1e-6, 1e-5],\n",
    "    'max_iter': [7, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "accuracy = best_estimator.score(X_test, y_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76b57332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 566, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 15, 'tol': 1e-07}\n",
      "Accuracy on Test Set: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "svm_model = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'C': [566, 79],\n",
    "    'gamma': ['scale', 'auto', 'custom'],\n",
    "    'tol': [1e-7, 1e-6, 1e-5],\n",
    "    'max_iter': [7, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "accuracy = best_estimator.score(X_test, y_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb2a3000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'algorithm': 'auto', 'n_neighbors': 78, 'p': 2, 'weights': 'distance'}\n",
      "Accuracy on Test Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [78],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "accuracy = best_estimator.score(X_test, y_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b8207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
